{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Koneoppiminen: Osa 3: Naiivi Bayes-luokittelija"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student_info"
    ]
   },
   "outputs": [],
   "source": [
    "# Kirjoita tähän tietosi!\n",
    "student_name = 'Samuli Kotiranta'\n",
    "student_id = 'AB8349'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tehtävä 1\n",
    "### Aihe: Artificial Characters Data Set (4 pistettä)\n",
    "\n",
    "1. Tutustu *Wine Data Set* -aineistoon osoitteessa https://archive.ics.uci.edu/ml/datasets/Wine .\n",
    "\n",
    "2. Lataa aineisto ja aseta sarakkeiden nimet. \n",
    "\n",
    "3. Jaa aineisto koulutus- ja testausaineistoon käyttäen `scikit-learn`-kirjaston [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)-funktiota.  \n",
    "\n",
    "4. Normalisoi sarakkeet välille 0...1 `scikit-learn`-kirjaston `preprocessing`-moduulin [minmax_scale](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.minmax_scale.html#sklearn.preprocessing.minmax_scale)-funktiolla.\n",
    "\n",
    "- Tallenna koulutusaineisto ja sen luokkamuuttuja muuttujiin `X_train`, `y_train`,\n",
    "- ja toisaalta testausaineiston `X_test`, `y_test`. \n",
    "\n",
    "Aseta testausjoukon kooksi 33% ja satunnaisuuden siemeneksi `1900`.\n",
    "\n",
    "Vinkki: älä skaalaa luokkamuuttujaa. \n",
    "\n",
    "Vinkki: muista että luokkamuuttuja ei mene `X`-muuttujiin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kirjoita toteutuksesi tähän soluun. \n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#luetaan data\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data'\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "#attribuutit\n",
    "#tämä on jatkoa ajatellen kätevä tapa\n",
    "attr = ['type',\n",
    "        'Alcohol',\n",
    "        'Malic acid',\n",
    "        'Ash',\n",
    "        'Alcalinity of ash',\n",
    "        'Magnesium',\n",
    "        'Total phenols',\n",
    "        'Flavanoids',\n",
    "        'Nonflavanoid phenols',\n",
    "        'Proanthocyanins',\n",
    "        'Color intensity',\n",
    "        'Hue',\n",
    "        'OD280/OD315 of diluted wines',\n",
    "        'Proline']\n",
    "df.columns = attr\n",
    "\n",
    "#luokkamuuttujan eristäminen ja aineiston splittaus\n",
    "X = df.drop(columns='type')\n",
    "y = df['type']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1900)\n",
    "\n",
    "#skaalaus ja aineiston jako\n",
    "mms = MinMaxScaler().fit(X_train)\n",
    "X_train = pd.DataFrame(mms.transform(X_train), columns=attr[1:])\n",
    "X_test = pd.DataFrame(mms.transform(X_test), columns=attr[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "answer_3_1"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train columns: Index(['Alcohol', 'Malic acid', 'Ash', 'Alcalinity of ash', 'Magnesium',\n",
      "       'Total phenols', 'Flavanoids', 'Nonflavanoid phenols',\n",
      "       'Proanthocyanins', 'Color intensity', 'Hue',\n",
      "       'OD280/OD315 of diluted wines', 'Proline'],\n",
      "      dtype='object')\n",
      "X_train:       Alcohol  Malic acid       Ash  Alcalinity of ash  Magnesium  \\\n",
      "0    0.112903    0.317227  0.566845           0.484536   0.282609   \n",
      "1    0.766129    0.871849  0.465241           0.484536   0.108696   \n",
      "2    0.478495    0.298319  0.556150           0.690722   0.304348   \n",
      "3    0.373656    0.743697  0.732620           0.819588   0.347826   \n",
      "4    0.575269    0.357143  0.540107           0.484536   0.543478   \n",
      "..        ...         ...       ...                ...        ...   \n",
      "113  0.615591    0.493697  0.545455           0.561856   0.239130   \n",
      "114  0.825269    0.237395  0.556150           0.422680   0.358696   \n",
      "115  0.709677    0.075630  0.299465           0.381443   0.260870   \n",
      "116  0.279570    0.266807  0.433155           0.536082   0.163043   \n",
      "117  0.879032    0.216387  0.727273           0.484536   0.543478   \n",
      "\n",
      "     Total phenols  Flavanoids  Nonflavanoid phenols  Proanthocyanins  \\\n",
      "0         0.662069    0.516878              0.358491         0.446203   \n",
      "1         0.000000    0.000000              0.509434         0.082278   \n",
      "2         0.058621    0.158228              0.264151         0.129747   \n",
      "3         0.420690    0.377637              0.566038         0.408228   \n",
      "4         0.231034    0.071730              0.754717         0.329114   \n",
      "..             ...         ...                   ...              ...   \n",
      "113       0.327586    0.088608              0.603774         0.262658   \n",
      "114       0.610345    0.544304              0.358491         0.620253   \n",
      "115       0.386207    0.305907              0.358491         0.098101   \n",
      "116       0.558621    0.487342              0.452830         0.294304   \n",
      "117       0.627586    0.590717              0.377358         0.490506   \n",
      "\n",
      "     Color intensity       Hue  OD280/OD315 of diluted wines   Proline  \n",
      "0           0.150849  0.260163                      0.776557  0.241007  \n",
      "1           0.315684  0.081301                      0.021978  0.089928  \n",
      "2           0.395604  0.146341                      0.032967  0.194245  \n",
      "3           0.033966  0.357724                      0.677656  0.053957  \n",
      "4           0.755245  0.097561                      0.128205  0.395683  \n",
      "..               ...       ...                           ...       ...  \n",
      "113         0.667333  0.056911                      0.128205  0.258993  \n",
      "114         0.445554  0.479675                      0.542125  0.553957  \n",
      "115         0.205794  0.609756                      0.435897  0.244604  \n",
      "116         0.101898  0.308943                      0.736264  0.063309  \n",
      "117         0.445554  0.479675                      0.505495  0.712230  \n",
      "\n",
      "[118 rows x 13 columns]\n",
      "X_test:      Alcohol  Malic acid       Ash  Alcalinity of ash  Magnesium  \\\n",
      "0   0.422043    0.329832  0.449198           0.407216   0.260870   \n",
      "1   0.360215    0.050420  0.427807           0.432990   0.184783   \n",
      "2   0.543011    0.184874  0.395722           0.329897   0.402174   \n",
      "3   0.260753    0.006303  0.342246           0.432990   0.173913   \n",
      "4   0.338710    0.151261  0.454545           0.505155   0.358696   \n",
      "5   0.314516    0.449580  0.513369           0.432990   0.282609   \n",
      "6   0.900538    0.205882  0.545455           0.072165   0.347826   \n",
      "7   0.666667    0.191176  0.689840           0.432990   0.434783   \n",
      "8   0.760753    0.096639  0.486631           0.278351   0.304348   \n",
      "9   0.373656    0.151261  0.443850           0.613402   0.413043   \n",
      "10  0.349462    0.044118  0.491979           0.278351   0.336957   \n",
      "11  0.491935    0.096639  0.513369           0.381443   0.565217   \n",
      "12  0.728495    0.163866  0.475936           0.298969   0.521739   \n",
      "13  0.543011    1.031513  0.411765           0.561856   0.173913   \n",
      "14  0.481183    0.521008  0.502674           0.458763   0.195652   \n",
      "15  0.403226    0.970588  0.684492           0.742268   0.282609   \n",
      "16  0.900538    0.567227  0.491979           0.278351   0.347826   \n",
      "17  0.701613    0.464286  0.641711           0.237113   0.500000   \n",
      "18  0.685484    0.161765  0.534759           0.438144   0.391304   \n",
      "19  0.637097    0.634454  0.598930           0.639175   0.347826   \n",
      "20  0.518817    0.537815  0.529412           0.407216   0.391304   \n",
      "21  0.833333    0.674370  0.737968           0.716495   0.282609   \n",
      "22  0.510753    0.611345  0.689840           0.412371   0.347826   \n",
      "23  0.739247    0.392857  0.502674           0.587629   0.217391   \n",
      "24  0.212366    0.174370  0.278075           0.458763   0.173913   \n",
      "25  0.661290    0.567227  0.443850           0.458763   0.195652   \n",
      "26  0.102151   -0.031513  0.609626           0.536082   0.195652   \n",
      "27  0.658602    0.193277  0.561497           0.510309   0.326087   \n",
      "28  0.543011    0.176471  0.363636           0.092784   0.239130   \n",
      "29  0.723118    0.113445  0.609626           0.314433   0.413043   \n",
      "30  0.209677    0.258403  0.737968           0.561856   0.695652   \n",
      "31  0.217742    0.420168  0.465241           0.381443   0.456522   \n",
      "32  0.494624    0.781513  0.598930           0.561856   0.173913   \n",
      "33  0.634409    0.184874  0.673797           0.283505   0.250000   \n",
      "34  0.731183    0.176471  0.561497           0.278351   0.206522   \n",
      "35  0.204301    0.260504  0.759358           0.922680   0.239130   \n",
      "36  0.543011    0.159664  0.636364           0.381443   0.304348   \n",
      "37  0.432796    0.098739  0.352941           0.319588   0.326087   \n",
      "38  1.021505    0.157563  0.433155           0.175258   0.293478   \n",
      "39  0.854839    0.661765  0.577540           0.427835   0.445652   \n",
      "40  0.349462    0.021008  0.315508           0.216495   0.717391   \n",
      "41  0.397849    0.073529  0.475936           0.355670   0.163043   \n",
      "42  0.733871    0.134454  0.716578           0.458763   0.673913   \n",
      "43  0.771505    0.165966  0.406417           0.278351   0.336957   \n",
      "44  0.889785    0.165966  0.716578           0.742268   0.304348   \n",
      "45  0.467742    0.533613  0.331551           0.278351   0.108696   \n",
      "46  0.725806    0.128151  0.716578           0.613402   0.336957   \n",
      "47  0.857527    0.651261  0.614973           0.134021   0.630435   \n",
      "48  0.575269    0.903361  0.513369           0.587629   0.250000   \n",
      "49  0.661290    0.161765  0.470588           0.690722   0.184783   \n",
      "50  0.857527    0.170168  0.502674           0.293814   0.521739   \n",
      "51  0.489247    0.500000  0.652406           0.587629   0.391304   \n",
      "52  0.760753    0.130252  0.700535           0.742268   0.173913   \n",
      "53  0.698925    0.193277  0.716578           0.340206   0.456522   \n",
      "54  0.626344    0.350840  0.529412           0.484536   0.206522   \n",
      "55  0.594086    0.357143  0.807487           0.536082   0.521739   \n",
      "56  0.497312    0.441176  0.556150           0.484536   0.369565   \n",
      "57  0.610215    0.174370  0.417112           0.329897   0.260870   \n",
      "58  0.260753    0.533613  0.342246           0.432990   0.184783   \n",
      "\n",
      "    Total phenols  Flavanoids  Nonflavanoid phenols  Proanthocyanins  \\\n",
      "0        0.220690    0.067511              0.943396         0.164557   \n",
      "1        0.868966    0.582278              0.113208         0.458861   \n",
      "2        0.696552    0.561181              0.283019         0.509494   \n",
      "3        0.496552    0.405063              0.320755         0.319620   \n",
      "4        0.041379    0.143460              0.452830         0.329114   \n",
      "5        0.093103    0.031646              0.509434         0.098101   \n",
      "6        0.800000    0.696203              0.301887         0.803797   \n",
      "7        0.472414    0.462025              0.301887         0.354430   \n",
      "8        0.689655    0.592827              0.169811         0.452532   \n",
      "9        0.351724    0.369198              0.396226         0.376582   \n",
      "10       0.368966    0.158228              0.943396        -0.003165   \n",
      "11       0.182759    0.191983              0.150943         0.164557   \n",
      "12       0.558621    0.540084              0.150943         0.379747   \n",
      "13       0.565517    0.487342              0.320755         0.503165   \n",
      "14       0.172414    0.067511              0.509434         0.174051   \n",
      "15       0.279310    0.054852              0.943396         0.215190   \n",
      "16       0.782759    0.597046              0.264151         0.560127   \n",
      "17       0.593103    0.567511              0.075472         0.392405   \n",
      "18       0.648276    0.601266              0.169811         0.484177   \n",
      "19       0.282759    0.086498              0.566038         0.313291   \n",
      "20       0.141379    0.075949              0.509434         0.164557   \n",
      "21       0.368966    0.088608              0.811321         0.294304   \n",
      "22       0.493103    0.436709              0.226415         0.493671   \n",
      "23       0.127586    0.071730              0.528302         0.193038   \n",
      "24       0.524138    0.274262              0.452830         0.316456   \n",
      "25       0.220690    0.029536              0.849057         0.145570   \n",
      "26       0.517241    0.352321              0.547170         0.322785   \n",
      "27       0.593103    0.556962              0.245283         0.455696   \n",
      "28       0.600000    0.618143              0.075472         0.787975   \n",
      "29       0.834483    0.702532              0.113208         0.512658   \n",
      "30       0.213793    0.137131              0.018868         0.360759   \n",
      "31       0.255172    0.206751              0.566038         0.167722   \n",
      "32       0.248276    0.065401              0.641509         0.139241   \n",
      "33       0.644828    0.548523              0.396226         0.325949   \n",
      "34       0.558621    0.510549              0.301887         0.439873   \n",
      "35       0.396552    0.400844              0.849057         0.424051   \n",
      "36       0.506897    0.440928              0.301887         0.322785   \n",
      "37       0.358621    0.225738              0.754717         0.063291   \n",
      "38       0.627586    0.556962              0.301887         0.493671   \n",
      "39       0.644828    0.487342              0.320755         0.262658   \n",
      "40       0.317241    0.318565              0.415094         0.740506   \n",
      "41       0.351724    0.050633              0.886792         0.262658   \n",
      "42       0.679310    0.506329              0.698113         0.294304   \n",
      "43       0.731034    0.643460              0.150943         0.544304   \n",
      "44       0.627586    0.204641              0.754717         0.721519   \n",
      "45       0.224138    0.191983              0.566038         0.129747   \n",
      "46       0.696552    0.613924              0.301887         0.620253   \n",
      "47       0.696552    0.569620              0.132075         0.525316   \n",
      "48       0.262069    0.061181              0.905660         0.357595   \n",
      "49       0.310345    0.316456              0.264151         0.193038   \n",
      "50       0.765517    0.561181              0.245283         0.509494   \n",
      "51       0.231034    0.054852              0.886792         0.170886   \n",
      "52       0.679310    0.531646              0.150943         0.458861   \n",
      "53       0.644828    0.542194              0.320755         0.329114   \n",
      "54       0.144828    0.033755              0.452830         0.069620   \n",
      "55       0.627586    0.495781              0.490566         0.443038   \n",
      "56       0.110345    0.185654              0.207547         0.129747   \n",
      "57       0.489655    0.390295              0.264151         0.294304   \n",
      "58       0.351724    0.274262              0.452830         0.458861   \n",
      "\n",
      "    Color intensity       Hue  OD280/OD315 of diluted wines   Proline  \n",
      "0          0.535465  0.203252                      0.113553  0.291367  \n",
      "1          0.270729  0.601626                      0.586081  0.093525  \n",
      "2          0.329670  0.325203                      0.761905  0.428058  \n",
      "3          0.075924  0.731707                      0.677656 -0.008633  \n",
      "4          0.130869  0.346341                      0.201465  0.417266  \n",
      "5          0.375624  0.146341                      0.205128  0.158273  \n",
      "6          0.575425  0.585366                      0.633700  0.904317  \n",
      "7          0.245754  0.504065                      0.586081  0.579137  \n",
      "8          0.547453  0.430894                      0.835165  0.543165  \n",
      "9          0.031968  0.471545                      0.619048  0.039568  \n",
      "10         0.152847  0.626016                      0.146520  0.280576  \n",
      "11         0.235764  0.227642                      0.007326  0.244604  \n",
      "12         0.410589  0.357724                      0.706960  0.553957  \n",
      "13         0.085914  0.203252                      0.670330  0.064748  \n",
      "14         0.851149  0.195122                      0.175824  0.284173  \n",
      "15         0.325674  0.276423                      0.153846  0.161871  \n",
      "16         0.315684  0.455285                      0.794872  0.557554  \n",
      "17         0.335664  0.390244                      0.765568  0.399281  \n",
      "18         0.515485  0.495935                      0.589744  0.881295  \n",
      "19         0.555445  0.178862                      0.106227  0.330935  \n",
      "20         0.353646  0.162602                      0.175824  0.276978  \n",
      "21         0.745255  0.105691                      0.120879  0.194245  \n",
      "22         0.275724  0.447154                      0.824176  0.345324  \n",
      "23         0.783217  0.178862                      0.150183  0.233813  \n",
      "24         0.031968  0.373984                      0.428571  0.089928  \n",
      "25         0.395604  0.268293                      0.201465  0.208633  \n",
      "26         0.133866  0.504065                      0.380952  0.103597  \n",
      "27         0.335664  0.455285                      0.805861  0.453237  \n",
      "28         0.545455  0.520325                      0.600733  0.618705  \n",
      "29         0.505495  0.333333                      0.586081  0.715827  \n",
      "30         0.075924  0.382114                      0.362637  0.241007  \n",
      "31         0.090909  0.390244                      0.457875  0.151079  \n",
      "32         0.590410  0.048780                      0.216117  0.241007  \n",
      "33         0.305694  0.357724                      0.714286  0.651079  \n",
      "34         0.385614  0.544715                      0.597070  0.741007  \n",
      "35         0.125874  0.398374                      0.428571  0.126619  \n",
      "36         0.250749  0.520325                      0.454212  0.586331  \n",
      "37         0.400599  0.406504                      0.117216  0.115108  \n",
      "38         0.345654  0.487805                      0.578755  0.543165  \n",
      "39         0.349650  0.317073                      0.754579  0.568345  \n",
      "40         0.165834  0.471545                      0.380952  0.330935  \n",
      "41         0.370629  0.219512                      0.087912  0.258993  \n",
      "42         0.365634  0.626016                      0.633700  0.679856  \n",
      "43         0.435564  0.349593                      0.754579  0.500000  \n",
      "44         1.124875  0.073171                      0.252747  0.266187  \n",
      "45         0.165834  0.178862                      0.311355  0.058993  \n",
      "46         0.395604  0.577236                      0.527473  0.715827  \n",
      "47         0.335664  0.333333                      0.827839  0.338129  \n",
      "48         0.615385  0.097561                      0.076923  0.312950  \n",
      "49         0.199800  0.406504                      0.553114  0.130935  \n",
      "50         0.463536  0.373984                      0.747253  0.489209  \n",
      "51         0.383616  0.317073                      0.307692  0.201439  \n",
      "52         0.163836  0.715447                      0.692308  0.086331  \n",
      "53         0.555445  0.650407                      0.589744  0.733813  \n",
      "54         0.385614  0.178862                      0.439560  0.352518  \n",
      "55         0.257742  0.455285                      0.608059  0.320144  \n",
      "56         0.365634  0.211382                      0.054945  0.172662  \n",
      "57         0.220779  0.439024                      0.549451  0.715827  \n",
      "58        -0.045954  0.365854                      0.652015  0.197122  \n"
     ]
    }
   ],
   "source": [
    "# Tehtävän vastaukset. Huom! Älä muokkaa tätä solua, vaan aja se, kun olet suorittanut tehtävän. \n",
    "# Tallenna vastauksesi X_train- X_test-muuttujiin.\n",
    "print(f'X_train columns: {X_train.columns}')\n",
    "print(f'X_train: {X_train}')\n",
    "print(f'X_test: {X_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# oma kommentti\n",
    "\n",
    "Tässä on nyt käytetty vielä kolmatta tapaa sarakkeiden nimeämiseen. Datasta pudotetaan aluksi luokkamuuttuja kokonaan pois ja sitten se jaetaan annettujen speksien mukaisesti pseudosatunnaisesti opetus- ja testiaineistoon. Vasta tämän jälkeen tehdään skaalaus, jolla kaikki opetus- ja testiaineiston arvot saadaan saman suuruusluokan asteikolle. Järjestys on tärkeä, jotta skaalauskertoimien kautta opetus- ja testiaineistoihin ei pääse vahingossa tihkumaan data-aineiston kokonaisuutena sisältämää informaatiota. Herra Fischer pysyy tyytyväisenä."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tehtävä 2\n",
    "### Aihe: Gaussian Naive Bayes (2 pistettä)\n",
    "\n",
    "Käytä `scikit-learn`-kirjastosta löytyvää *Gaussian Naive Bayes* -menetelmää aineiston luokittelemiseksi.\n",
    "\n",
    "Ennnusta testiaineiston luokka. Tallenna ennusteet `y_pred`-muuttujaan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kirjoita toteutuksesi tähän soluun. \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "#tässä on nyt hyvin kompakti muotoilu\n",
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(X_train, y_train).predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "answer_3_2"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n"
     ]
    }
   ],
   "source": [
    "# Tehtävän vastaukset. Huom! Älä muokkaa tätä solua, vaan aja se, kun olet suorittanut tehtävän. \n",
    "# Tallenna ennusteen tulokset y_pred-muuttujaan.\n",
    "print(f'{len(y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# oma kommentti\n",
    "\n",
    "Tässä käytetty, sanalla sanoen erittäin kompakti, muotoilu luokittelijalle taisi löytyä esimerkeistä. Tässä siis opetusaineiston perusteella tehdyn sovituksen jälkeen tehdään suoraan ennuste testiaineistolle. Neljä vuotta bayesin kanssa työskennelleenä voisin kirjoittaa tähän menetelmän matemaattisesta perustasta paljonkin. Jääköön se nyt kuitenkin toiseen kertaan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tehtävä 3\n",
    "### Aihe: Tarkkuuden mittaaminen (4 pistettä)\n",
    "\n",
    "Hyödynnä `scikit-learn`-kirjaston `metrics`-moduulin funktioita ja laske luokittelutuloksesi **tarkkuus** sekä **sekaannusmatriisi**. Tallenna tarkkuus muuttujaan `acc` ja sekaannusmatriisi muuttujaan `cm`. \n",
    "\n",
    "Vinkki: onnistumista voit mitata ainoastaa testiaineiston osalta!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Kirjoita toteutuksesi tähän soluun. \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#ennusteen tarkkuus ja sekaannusmatriisi\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "import seaborn as sns\n",
    "sns.heatmap(cm, annot=True, cmap='Reds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "answer_3_3"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9661016949152542\n",
      "Confusion matrix:\n",
      " [[24  1  0]\n",
      " [ 0 16  1]\n",
      " [ 0  0 17]]\n"
     ]
    }
   ],
   "source": [
    "# Tehtävän vastaukset. Huom! Älä muokkaa tätä solua, vaan aja se, kun olet suorittanut tehtävän. \n",
    "# Tallenna vastauksesi acc- ja cm-muuttujiin.\n",
    "print(f'Accuracy: {acc}')\n",
    "print(f'Confusion matrix:\\n {cm}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# oma kommentti\n",
    "\n",
    "Tässä lasketaan ennusteen tarkkuus valmiilla funktioilla vertaamalla ennustetta `y_pred` tähän asti erillään pidettyyn testiaineiston oikeaan arvoon `y_test`. Tarkkuus osoittautuu olevan aivan hyvä (mikä vanhaa bayesistia tietenkin miellyttää suuresti). Sekaannusmatriisin diagonaalilta näkyvät oikeaan osuneiden ennusteiden määrät kussakin luokassa. Siitä myös näkyy, kuinka pieleen menneitä ennusteita on vain kaksi kappaletta, molemmat matriisin yläkolmiossa. Viihtyvyyden lisäämiseksi matriisi on esitetty heatmappina. Punainen oli päivän väri."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
